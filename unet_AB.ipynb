{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgnarag/painting_restoration/blob/main/unet_AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gXM5CNP5BaYU",
        "outputId": "2743aade-5618-46ea-a09f-cf2238d0aed3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/My\\ Drive\n",
        "file_path = \"/content/drive/MyDrive/Baumgartner screenshots/\""
      ],
      "metadata": {
        "id": "xiAgOwvmBcjr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing packages"
      ],
      "metadata": {
        "id": "zEoJQzJ5bM8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "import cv2\n",
        "from skimage import color\n",
        "\n",
        "from tensorflow import keras\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import PIL\n",
        "PIL.Image.MAX_IMAGE_PIXELS = 236958876"
      ],
      "metadata": {
        "id": "FgE9bH4-MFlB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining functions"
      ],
      "metadata": {
        "id": "ihq928NMbTBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zoom_and_resize(img, zoom_factor):\n",
        "    height, width = img.shape[:2]\n",
        "    crop_top = int((height - height / zoom_factor) / 2)# Calculate the region to crop around the center\n",
        "    crop_bottom = int(height - crop_top)\n",
        "    crop_left = int((width - width / zoom_factor) / 2)\n",
        "    crop_right = int(width - crop_left)\n",
        "    cropped_image = img[crop_top:crop_bottom, crop_left:crop_right]    # Crop the image\n",
        "    resized_image = cv2.resize(cropped_image, (width, height))# Resize the zoomed image back to the original dimensions\n",
        "    return resized_image\n",
        "\n",
        "def crop(im):\n",
        "    width, height,depth = im.shape\n",
        "    data = []\n",
        "    step = int(size * 0.80)  # Adjust the step size for cropping\n",
        "    for j in range(0, height, step):\n",
        "        for i in range(0, width, step):\n",
        "            if i + size <= width and j + size <= height:\n",
        "                #im1 = im.crop((i, j, i + size, j + size))\n",
        "                im1 = im[i:i + size, j:j + size]\n",
        "                im1 = np.array(im1).astype(np.float32)\n",
        "                data.append(im1/255)\n",
        "                im1 = np.rot90(im1)\n",
        "                data.append(im1/255)\n",
        "                im1 = np.rot90(im1)\n",
        "                data.append(im1/255)\n",
        "                im1 = np.rot90(im1)\n",
        "                data.append(im1/255)\n",
        "                for z in zoom_factor:\n",
        "                    zoomed_img = zoom_and_resize(im1, z)\n",
        "                    data.append(zoomed_img/255)\n",
        "    return data\n",
        "\n",
        "def rgb_AB_normalized(im):\n",
        "    rgb_array = np.array(im) # Convert to numpy array\n",
        "    rgb_array_normalized = rgb_array / 255.0 # Normalize RGB values from 0 to 1\n",
        "    lab_array_normalized = color.rgb2lab(rgb_array_normalized) # Convert RGB to LAB colorspace\n",
        "    lab_array_normalized[..., 1:] = (lab_array_normalized[..., 1:] + 128) / 255.0# Scale A and B channels to range [0, 1]\n",
        "    ab_array_normalized = lab_array_normalized[..., 1:]# Create a new array with only AB channels\n",
        "    ab_image_normalized = Image.fromarray((ab_array_normalized * 255).astype(np.uint8))# Convert AB array back to image\n",
        "    return ab_array_normalized"
      ],
      "metadata": {
        "id": "GRBwljTxNi-i"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the training dataset"
      ],
      "metadata": {
        "id": "Nhjd4suVbXAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size = 32\n",
        "model_number = size\n",
        "\n",
        "zoom_factor = [1.50, 2.0, 2.50, 3.0]\n",
        "data = []\n",
        "\n",
        "input = Image.open(file_path + \"Training/TRAIN_INPUT_[04] Coming into Bloom.png\").convert('RGB')\n",
        "input = np.array(crop(rgb_AB_normalized(input)))\n",
        "print(\"Done reading the input of size = \", input.shape)\n",
        "\n",
        "output = Image.open(file_path + \"Training/TRAIN_OUTPUT_[04] Coming into Bloom.png\").convert('RGB')\n",
        "output = np.array(crop(rgb_AB_normalized(output)))\n",
        "print(\"Done reading the output of size = \", output.shape)\n",
        "\n",
        "train_input = []\n",
        "test_input = []\n",
        "for i in range(0,len(input)):\n",
        "    if i % 4 == 0:\n",
        "        test_input.append(input[i])\n",
        "    else:\n",
        "        train_input.append(input[i])\n",
        "\n",
        "train_input = np.array(train_input)\n",
        "test_input = np.array(test_input)\n",
        "\n",
        "print(\"Done reading the train input of size = \", train_input.shape)\n",
        "print(\"Done reading the test input of size = \", test_input.shape)\n",
        "input = []\n",
        "\n",
        "train_output = []\n",
        "test_output = []\n",
        "for i in range(0,len(output)):\n",
        "    if i % 4 == 0:\n",
        "        test_output.append(output[i])\n",
        "    else:\n",
        "        train_output.append(output[i])\n",
        "\n",
        "train_output = np.array(train_output)\n",
        "test_output = np.array(test_output)\n",
        "\n",
        "print(\"Done reading the train output of size = \", train_input.shape)\n",
        "print(\"Done reading the test output of size = \", test_input.shape)\n",
        "\n",
        "output = []"
      ],
      "metadata": {
        "id": "MjpPVdDyBeby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5e52fc-b384-476f-988c-1379ce2a2bb6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done reading the input of size =  (10600, 32, 32, 2)\n",
            "Done reading the output of size =  (10600, 32, 32, 2)\n",
            "Done reading the train input of size =  (7950, 32, 32, 2)\n",
            "Done reading the test input of size =  (2650, 32, 32, 2)\n",
            "Done reading the train output of size =  (7950, 32, 32, 2)\n",
            "Done reading the test output of size =  (2650, 32, 32, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N = 1000\n",
        "n = 10\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(0,10):\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.title(\"input\")\n",
        "    plt.imshow(train_input[i + N].reshape(32, 32), cmap='jet')\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    bx = plt.subplot(2, n, i + n + 1)\n",
        "    plt.title(\"output\")\n",
        "    plt.imshow(train_output[i + N].reshape(32, 32), cmap='jet')\n",
        "    plt.gray()\n",
        "    bx.get_xaxis().set_visible(False)\n",
        "    bx.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zUKPw12vBfw_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "eb25209b-ed71-4a1a-83c2-2459c2cc37d2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 2048 into shape (32,32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3ef57472b5f4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 2048 into shape (32,32)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Unet model"
      ],
      "metadata": {
        "id": "dVinHDkbcBXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization\n",
        "from tensorflow.keras import layers, losses\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
        "\n",
        "def unet_model(input_shape=(size, size, 3)):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Contracting Path\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same', strides = 2)(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same', strides = 2)(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "\n",
        "    #conv4 = Conv2D(512, 3, activation='relu', padding='same', strides = 2)(conv3)\n",
        "    #conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
        "\n",
        "    #conv5 = concatenate([Conv2DTranspose(256, kernel_size=3, strides=2, activation='relu', padding='same')(conv4), conv3], axis = -1)\n",
        "    #conv5 = Conv2D(128, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    conv6 = concatenate([Conv2DTranspose(128, kernel_size=3, strides=2, activation='relu', padding='same')(conv3), conv2], axis = -1)\n",
        "    conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    conv7 = concatenate([Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu', padding='same')(conv6), conv1], axis = -1)\n",
        "    conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    # Output Layer\n",
        "    output = Conv2D(3, kernel_size=(3, 3), activation='sigmoid', padding='same')(conv7)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "# Usage example:\n",
        "unet = unet_model()\n",
        "unet.summary()"
      ],
      "metadata": {
        "id": "3zLL3Bc6DpPz",
        "outputId": "70fc1729-13b8-4747-ffa7-d7f9e9fc5b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 32, 32, 64)           1792      ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 32, 32, 64)           36928     ['conv2d_12[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 16, 16, 128)          73856     ['conv2d_13[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)          (None, 16, 16, 128)          147584    ['conv2d_14[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)          (None, 8, 8, 256)            295168    ['conv2d_15[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)          (None, 8, 8, 256)            590080    ['conv2d_16[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2D  (None, 16, 16, 128)          295040    ['conv2d_17[0][0]']           \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 16, 16, 256)          0         ['conv2d_transpose_3[0][0]',  \n",
            " )                                                                   'conv2d_15[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 16, 16, 128)          295040    ['concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_transpose_4 (Conv2D  (None, 32, 32, 64)           73792     ['conv2d_18[0][0]']           \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate  (None, 32, 32, 128)          0         ['conv2d_transpose_4[0][0]',  \n",
            " )                                                                   'conv2d_13[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)          (None, 32, 32, 64)           73792     ['concatenate_4[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)          (None, 32, 32, 3)            1731      ['conv2d_19[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1884803 (7.19 MB)\n",
            "Trainable params: 1884803 (7.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Unet model"
      ],
      "metadata": {
        "id": "Ni8MyrOhcE-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "unet.compile(optimizer=opt, loss=losses.MeanSquaredError())\n",
        "start = time.time()\n",
        "model = unet.fit(train_input, train_output,\n",
        "                epochs=100,\n",
        "                shuffle=True,\n",
        "                validation_data=(test_input, test_output),batch_size=16)\n",
        "end = time.time()\n",
        "\n",
        "print(\"The time of execution of above program is :\",\n",
        "      (end-start), \"seconds\")\n",
        "\n",
        "unet.save(file_path + 'Model/unet_' + str(model_number) + '_lab')\n",
        "\n",
        "loss = model.history['loss']\n",
        "loss = pd.DataFrame(loss)\n",
        "loss.to_csv(file_path + 'Model/loss_'+ str(model_number) + '_lab.csv')\n",
        "val_loss = model.history['val_loss']\n",
        "val_loss = pd.DataFrame(val_loss)\n",
        "val_loss.to_csv(file_path  +'Model/val_loss_'+ str(model_number)+ '_lab.csv')"
      ],
      "metadata": {
        "id": "ZaMd_O_LN6pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model"
      ],
      "metadata": {
        "id": "3lnB29-_cVp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import PIL\n",
        "PIL.Image.MAX_IMAGE_PIXELS = 236958876\n",
        "\n",
        "n_size = 32\n",
        "\n",
        "def lab_rgb_unnormalized(lab_image):\n",
        "    lab_array = np.array(lab_image/255)# Convert LAB image to numpy array\n",
        "    lab_array[..., 0] = lab_array[..., 0] * 100.0# Scale LAB values back to their original ranges\n",
        "    lab_array[..., 1] = (lab_array[..., 1] * 255.0) - 128\n",
        "    lab_array[..., 2] = (lab_array[..., 2] * 255.0) - 128\n",
        "    rgb_array_unnormalized = color.lab2rgb(lab_array)# Convert LAB to RGB colorspace\n",
        "    rgb_array_unnormalized = (rgb_array_unnormalized * 255).astype(np.uint8)# Scale RGB values back to the range [0, 255]\n",
        "    rgb_image_unnormalized = Image.fromarray(rgb_array_unnormalized, mode='RGB')# Convert RGB array back to image\n",
        "    return rgb_image_unnormalized\n",
        "\n",
        "def AB_rgb_unnormalized(lab_image, test_L):\n",
        "    lab_array = np.array(lab_image/255)# Convert LAB image to numpy array\n",
        "    lab_array[..., 0] = test_L #replacing L from the original input\n",
        "    lab_array[..., 1] = (lab_array[..., 1] * 255.0) - 128\n",
        "    lab_array[..., 2] = (lab_array[..., 2] * 255.0) - 128\n",
        "    rgb_array_unnormalized = color.lab2rgb(lab_array)# Convert LAB to RGB colorspace\n",
        "    rgb_array_unnormalized = (rgb_array_unnormalized * 255).astype(np.uint8)# Scale RGB values back to the range [0, 255]\n",
        "    rgb_image_unnormalized = Image.fromarray(rgb_array_unnormalized, mode='RGB')# Convert RGB array back to image\n",
        "    return rgb_image_unnormalized\n",
        "\n",
        "def crop_(im):\n",
        "    width, height = im.size\n",
        "    data = []\n",
        "    for j in range(0,int(height/n_size)):\n",
        "        for i in range(0,int(width/n_size)):\n",
        "            im1 = im.crop((0 + (n_size*i), 0 + (n_size*j), n_size + (n_size*i), n_size + (n_size*j)))\n",
        "            im1 = np.array(im1)\n",
        "            im1 = im1.astype(np.float32)\n",
        "            im1 = im1/255\n",
        "            data.append(im1)\n",
        "    return data\n",
        "\n",
        "\n",
        "#----------------READING THE TEST IMAGE--------------------#\n",
        "filename = str(\"TEST_INPUT_[04] Coming into Bloom_V1.png\")\n",
        "test = Image.open(file_path+ \"Testing/\" + filename).convert('RGB')\n",
        "test_L = color.rgb2lab(np.array(test))\n",
        "test_L = test_L[..., 0]\n",
        "test = rgb_lab_normalized(test)\n",
        "w_dirty, h_dirty = test.size\n",
        "#----------------------------------------------------------#\n",
        "\n",
        "autoencoder = keras.models.load_model(file_path + 'Model/unet_'+str(n_size)+'_lab')\n",
        "\n",
        "\n",
        "xx = int(w_dirty/n_size)\n",
        "final=[]\n",
        "for portion in range(0,xx):\n",
        "    im1 = test.crop((n_size*portion, 0, (n_size*portion) + n_size, h_dirty))\n",
        "    w1, h1 = im1.size\n",
        "    w = int(w1/n_size)\n",
        "    h = int(h1/n_size)\n",
        "    neverbeforeseen = np.array(crop_(im1))\n",
        "    decoded_imgs = autoencoder.predict(neverbeforeseen)\n",
        "    col = np.vstack((decoded_imgs[0],decoded_imgs[1]))\n",
        "    for i in range(2,h):\n",
        "        col = np.vstack((col,decoded_imgs[i]))\n",
        "    y = col\n",
        "    y = (y * 255).astype('uint8')\n",
        "    if portion == 0:\n",
        "        final = y\n",
        "    if portion > 0:\n",
        "        final = np.hstack((final,y))\n",
        "\n",
        "#----------------TEST ON FULL LAB--------------------#\n",
        "final_LAB = lab_rgb_unnormalized(final)\n",
        "final_LAB = np.squeeze(final_LAB)\n",
        "reconstructed_LAB = Image.fromarray(final_LAB)\n",
        "reconstructed_LAB.save(file_path + \"Testing/\"+\"CLEAN\"+str(size)+\"LAB_\"+filename)\n",
        "plt.figure(), plt.imshow(reconstructed_LAB)\n",
        "\n",
        "#----------------TEST ON L from input and AB from model--------------------#\n",
        "final_L_AB = AB_rgb_unnormalized(final, test_L)\n",
        "final_L_AB = np.squeeze(final_L_AB)\n",
        "reconstructed_L_AB = Image.fromarray(final_L_AB)\n",
        "reconstructed_L_AB.save(file_path + \"Testing/\"+\"CLEAN\"+str(size)+\"L_AB_\"+filename)\n",
        "plt.figure(), plt.imshow(reconstructed_L_AB)"
      ],
      "metadata": {
        "id": "y3j1v9uGdbTb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}