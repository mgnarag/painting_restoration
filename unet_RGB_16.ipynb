{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgnarag/painting_restoration/blob/main/unet_RGB_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gXM5CNP5BaYU",
        "outputId": "a80b37c2-b2e3-4f14-829d-c91ebe30a89d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/My\\ Drive\n",
        "file_path = \"/content/drive/MyDrive/Baumgartner screenshots/\""
      ],
      "metadata": {
        "id": "xiAgOwvmBcjr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing packages"
      ],
      "metadata": {
        "id": "zEoJQzJ5bM8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "import cv2\n",
        "from skimage import color\n",
        "\n",
        "from tensorflow import keras\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import PIL\n",
        "PIL.Image.MAX_IMAGE_PIXELS = 236958876\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "FgE9bH4-MFlB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining functions"
      ],
      "metadata": {
        "id": "ihq928NMbTBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zoom_and_resize(img, zoom_factor):\n",
        "    height, width = img.shape[:2]\n",
        "    crop_top = int((height - height / zoom_factor) / 2)# Calculate the region to crop around the center\n",
        "    crop_bottom = int(height - crop_top)\n",
        "    crop_left = int((width - width / zoom_factor) / 2)\n",
        "    crop_right = int(width - crop_left)\n",
        "    cropped_image = img[crop_top:crop_bottom, crop_left:crop_right]    # Crop the image\n",
        "    resized_image = cv2.resize(cropped_image, (width, height))# Resize the zoomed image back to the original dimensions\n",
        "    return resized_image\n",
        "\n",
        "def crop(im):\n",
        "    width, height = im.size\n",
        "    data = []\n",
        "    step = int(size * 0.80)  # Adjust the step size for cropping\n",
        "    for j in range(0, height, step):\n",
        "        for i in range(0, width, step):\n",
        "            if i + size <= width and j + size <= height:\n",
        "                im1 = im.crop((i, j, i + size, j + size))\n",
        "                im1 = np.array(im1).astype(np.float32)\n",
        "                data.append(im1/255)\n",
        "                #im1 = np.rot90(im1)\n",
        "                #data.append(im1/255)\n",
        "                #im1 = np.rot90(im1)\n",
        "                #data.append(im1/255)\n",
        "                #im1 = np.rot90(im1)\n",
        "                #data.append(im1/255)\n",
        "                for z in zoom_factor:\n",
        "                    zoomed_img = zoom_and_resize(im1, z)\n",
        "                    data.append(zoomed_img/255)\n",
        "    return data\n",
        "\n",
        "def contrast_stretch(image):\n",
        "    img_array = np.array(image)\n",
        "    for i in range(3):  # 3 channels: Red, Green, Blue\n",
        "        # Compute min and max pixel intensity values for the channel\n",
        "        min_val = np.min(img_array[:,:,i])\n",
        "        max_val = np.max(img_array[:,:,i])\n",
        "        # Apply contrast stretching\n",
        "        img_array[:,:,i] = (img_array[:,:,i] - min_val) * (255.0 / (max_val - min_val))\n",
        "    # Convert numpy array back to PIL Image\n",
        "    stretched_image = Image.fromarray(np.uint8(img_array))\n",
        "    return stretched_image"
      ],
      "metadata": {
        "id": "GRBwljTxNi-i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the training dataset"
      ],
      "metadata": {
        "id": "Nhjd4suVbXAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size = 16\n",
        "model_number = size\n",
        "\n",
        "zoom_factor = [1.5,2.0]\n",
        "input = []\n",
        "output = []\n",
        "\n",
        "input_folder_path = file_path + 'Training/input/'\n",
        "output_folder_path = file_path + 'Training/output/'\n",
        "\n",
        "# List all files in the directory and sort them alphabetically\n",
        "input_files = sorted(os.listdir(input_folder_path))\n",
        "output_files = sorted(os.listdir(output_folder_path))\n",
        "\n",
        "\n",
        "for image_file in input_files:\n",
        "    image_path = os.path.join(input_folder_path, image_file)\n",
        "    image = contrast_stretch(Image.open(image_path).convert('RGB'))\n",
        "    image = crop(image)\n",
        "    input.extend(image)\n",
        "\n",
        "for image_file in output_files:\n",
        "    image_path = os.path.join(output_folder_path, image_file)\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = crop(image)\n",
        "    output.extend(image)\n",
        "input = np.array(input)\n",
        "output = np.array(output)\n",
        "\n",
        "print(\"Done reading the input of size = \", input.shape)\n",
        "print(\"Done reading the output of size = \", output.shape)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_input, test_input, train_output, test_output = train_test_split(\n",
        "    input, output, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Done reading the train input of size = \", train_input.shape)\n",
        "print(\"Done reading the test input of size = \", test_input.shape)\n",
        "print(\"Done reading the train output of size = \", train_input.shape)\n",
        "print(\"Done reading the test output of size = \", test_input.shape)\n",
        "\n",
        "input = []\n",
        "output = []"
      ],
      "metadata": {
        "id": "MjpPVdDyBeby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e50f7b-1034-4438-8757-2f8bbfa5ecac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done reading the input of size =  (109971, 16, 16, 3)\n",
            "Done reading the output of size =  (109971, 16, 16, 3)\n",
            "Done reading the train input of size =  (87976, 16, 16, 3)\n",
            "Done reading the test input of size =  (21995, 16, 16, 3)\n",
            "Done reading the train output of size =  (87976, 16, 16, 3)\n",
            "Done reading the test output of size =  (21995, 16, 16, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10000\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(0,10):\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.title(\"input\")\n",
        "    plt.imshow((train_input[i+N]))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    bx = plt.subplot(2, n, i + n + 1)\n",
        "    plt.title(\"output\")\n",
        "    plt.imshow((train_output[i+N]))\n",
        "    plt.gray()\n",
        "    bx.get_xaxis().set_visible(False)\n",
        "    bx.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IgvxE7_Ghuge",
        "outputId": "e95b4a9e-83ad-4a9c-84d3-23d23d1eae98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x400 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Unet model"
      ],
      "metadata": {
        "id": "dVinHDkbcBXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "# SSIM loss function\n",
        "def ssim_loss(y_true, y_pred):\n",
        "    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
        "\n",
        "def unet_model(input_shape=(size, size, 3)):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Contracting Path\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n",
        "    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same', strides=2)(conv1)\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n",
        "    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n",
        "\n",
        "    #conv3 = Conv2D(32, 3, activation='relu', padding='same', strides=2)(conv2)\n",
        "    #conv3 = Conv2D(32, 3, activation='relu', padding='same')(conv3)\n",
        "\n",
        "    #conv4 = Conv2D(64, 3, activation='relu', padding='same', strides=2)(conv3)\n",
        "    #conv4 = Conv2D(64, 3, activation='relu', padding='same')(conv4)\n",
        "\n",
        "    #conv5 = concatenate([Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same')(conv4), conv3], axis=-1)\n",
        "    #conv5 = Conv2D(32, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    #conv6 = concatenate([Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same')(conv3), conv2], axis=-1)\n",
        "    #conv6 = Conv2D(16, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    conv7 = concatenate([Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same')(conv2), conv1], axis=-1)\n",
        "    conv7 = Conv2D(32, 3, activation='relu', padding='same')(conv7)\n",
        "    conv7 = Conv2D(32, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    # Output Layer\n",
        "    output = Conv2D(3, kernel_size=(3, 3), activation='sigmoid', padding='same')(conv7)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "# Usage example:\n",
        "unet = unet_model()\n",
        "unet.summary()"
      ],
      "metadata": {
        "id": "uD5UP4JLi9MF",
        "outputId": "9da24651-29d7-4af5-e2af-aa77f833f1e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 16, 16, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)          (None, 16, 16, 32)           896       ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)          (None, 16, 16, 32)           9248      ['conv2d_18[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)          (None, 16, 16, 32)           9248      ['conv2d_19[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)          (None, 8, 8, 64)             18496     ['conv2d_20[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)          (None, 8, 8, 64)             36928     ['conv2d_21[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)          (None, 8, 8, 64)             36928     ['conv2d_22[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, 16, 16, 32)           18464     ['conv2d_23[0][0]']           \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 16, 16, 64)           0         ['conv2d_transpose_2[0][0]',  \n",
            " )                                                                   'conv2d_20[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)          (None, 16, 16, 32)           18464     ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)          (None, 16, 16, 32)           9248      ['conv2d_24[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)          (None, 16, 16, 3)            867       ['conv2d_25[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 158787 (620.26 KB)\n",
            "Trainable params: 158787 (620.26 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Unet model"
      ],
      "metadata": {
        "id": "Ni8MyrOhcE-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras import layers, losses\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "unet.compile(optimizer=opt, loss=ssim_loss)  # Using SSIM loss function\n",
        "#unet.compile(optimizer=opt, loss=losses.MeanSquaredError())\n",
        "# Train your model as before\n",
        "start = time.time()\n",
        "model = unet.fit(train_input, train_output,\n",
        "                epochs=250,\n",
        "                shuffle=True,\n",
        "                validation_data=(test_input, test_output),batch_size=32)\n",
        "end = time.time()\n",
        "\n",
        "print(\"The time of execution of above program is :\",\n",
        "      (end-start), \"seconds\")\n",
        "\n",
        "unet.save(file_path + 'Model/unet_' + str(model_number) + '_rgb')\n",
        "\n",
        "loss = model.history['loss']\n",
        "loss = pd.DataFrame(loss)\n",
        "loss.to_csv(file_path + 'Model/loss_'+ str(model_number) + '_rgb.csv')\n",
        "val_loss = model.history['val_loss']\n",
        "val_loss = pd.DataFrame(val_loss)\n",
        "val_loss.to_csv(file_path  +'Model/val_loss_'+ str(model_number)+ '_rgb.csv')"
      ],
      "metadata": {
        "id": "ZaMd_O_LN6pB",
        "outputId": "f4650baa-a754-4499-f69c-f7d190981141",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "2750/2750 [==============================] - 31s 9ms/step - loss: 0.1225 - val_loss: 0.1231\n",
            "Epoch 2/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.1211 - val_loss: 0.1217\n",
            "Epoch 3/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1200 - val_loss: 0.1200\n",
            "Epoch 4/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1190 - val_loss: 0.1197\n",
            "Epoch 5/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1183 - val_loss: 0.1194\n",
            "Epoch 6/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1173 - val_loss: 0.1186\n",
            "Epoch 7/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.1165 - val_loss: 0.1180\n",
            "Epoch 8/250\n",
            "2750/2750 [==============================] - 22s 8ms/step - loss: 0.1159 - val_loss: 0.1168\n",
            "Epoch 9/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1154 - val_loss: 0.1163\n",
            "Epoch 10/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1146 - val_loss: 0.1156\n",
            "Epoch 11/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.1141 - val_loss: 0.1155\n",
            "Epoch 12/250\n",
            "2750/2750 [==============================] - 23s 9ms/step - loss: 0.1135 - val_loss: 0.1168\n",
            "Epoch 13/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1131 - val_loss: 0.1148\n",
            "Epoch 14/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1125 - val_loss: 0.1141\n",
            "Epoch 15/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1120 - val_loss: 0.1145\n",
            "Epoch 16/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1116 - val_loss: 0.1140\n",
            "Epoch 17/250\n",
            "2750/2750 [==============================] - 23s 9ms/step - loss: 0.1111 - val_loss: 0.1140\n",
            "Epoch 18/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1107 - val_loss: 0.1146\n",
            "Epoch 19/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.1104 - val_loss: 0.1133\n",
            "Epoch 20/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1098 - val_loss: 0.1121\n",
            "Epoch 21/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.1096 - val_loss: 0.1132\n",
            "Epoch 22/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1092 - val_loss: 0.1124\n",
            "Epoch 23/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1088 - val_loss: 0.1133\n",
            "Epoch 24/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1085 - val_loss: 0.1126\n",
            "Epoch 25/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1081 - val_loss: 0.1119\n",
            "Epoch 26/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1078 - val_loss: 0.1119\n",
            "Epoch 27/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1075 - val_loss: 0.1125\n",
            "Epoch 28/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.1072 - val_loss: 0.1112\n",
            "Epoch 29/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1068 - val_loss: 0.1115\n",
            "Epoch 30/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.1066 - val_loss: 0.1119\n",
            "Epoch 31/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.1063 - val_loss: 0.1114\n",
            "Epoch 32/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.1060 - val_loss: 0.1104\n",
            "Epoch 33/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1057 - val_loss: 0.1103\n",
            "Epoch 34/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1054 - val_loss: 0.1103\n",
            "Epoch 35/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.1051 - val_loss: 0.1104\n",
            "Epoch 36/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1049 - val_loss: 0.1118\n",
            "Epoch 37/250\n",
            "2750/2750 [==============================] - 23s 9ms/step - loss: 0.1046 - val_loss: 0.1115\n",
            "Epoch 38/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1045 - val_loss: 0.1094\n",
            "Epoch 39/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1042 - val_loss: 0.1098\n",
            "Epoch 40/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.1039 - val_loss: 0.1101\n",
            "Epoch 41/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.1037 - val_loss: 0.1092\n",
            "Epoch 42/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1034 - val_loss: 0.1095\n",
            "Epoch 43/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.1032 - val_loss: 0.1101\n",
            "Epoch 44/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1030 - val_loss: 0.1099\n",
            "Epoch 45/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1029 - val_loss: 0.1092\n",
            "Epoch 46/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.1026 - val_loss: 0.1094\n",
            "Epoch 47/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.1024 - val_loss: 0.1096\n",
            "Epoch 48/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1022 - val_loss: 0.1089\n",
            "Epoch 49/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1019 - val_loss: 0.1088\n",
            "Epoch 50/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.1018 - val_loss: 0.1091\n",
            "Epoch 51/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1015 - val_loss: 0.1082\n",
            "Epoch 52/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1014 - val_loss: 0.1090\n",
            "Epoch 53/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.1012 - val_loss: 0.1091\n",
            "Epoch 54/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1010 - val_loss: 0.1084\n",
            "Epoch 55/250\n",
            "2750/2750 [==============================] - 29s 11ms/step - loss: 0.1009 - val_loss: 0.1094\n",
            "Epoch 56/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1006 - val_loss: 0.1083\n",
            "Epoch 57/250\n",
            "2750/2750 [==============================] - 23s 9ms/step - loss: 0.1004 - val_loss: 0.1103\n",
            "Epoch 58/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.1003 - val_loss: 0.1092\n",
            "Epoch 59/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.1001 - val_loss: 0.1084\n",
            "Epoch 60/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0999 - val_loss: 0.1087\n",
            "Epoch 61/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.0999 - val_loss: 0.1083\n",
            "Epoch 62/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0995 - val_loss: 0.1084\n",
            "Epoch 63/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0994 - val_loss: 0.1081\n",
            "Epoch 64/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.0992 - val_loss: 0.1081\n",
            "Epoch 65/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0991 - val_loss: 0.1074\n",
            "Epoch 66/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0990 - val_loss: 0.1093\n",
            "Epoch 67/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0987 - val_loss: 0.1076\n",
            "Epoch 68/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0986 - val_loss: 0.1079\n",
            "Epoch 69/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0984 - val_loss: 0.1078\n",
            "Epoch 70/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0983 - val_loss: 0.1094\n",
            "Epoch 71/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0981 - val_loss: 0.1078\n",
            "Epoch 72/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0980 - val_loss: 0.1074\n",
            "Epoch 73/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0978 - val_loss: 0.1080\n",
            "Epoch 74/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0976 - val_loss: 0.1076\n",
            "Epoch 75/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0975 - val_loss: 0.1072\n",
            "Epoch 76/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0974 - val_loss: 0.1071\n",
            "Epoch 77/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0972 - val_loss: 0.1073\n",
            "Epoch 78/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0972 - val_loss: 0.1069\n",
            "Epoch 79/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0969 - val_loss: 0.1072\n",
            "Epoch 80/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0969 - val_loss: 0.1073\n",
            "Epoch 81/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0968 - val_loss: 0.1071\n",
            "Epoch 82/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.0967 - val_loss: 0.1071\n",
            "Epoch 83/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0964 - val_loss: 0.1073\n",
            "Epoch 84/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0964 - val_loss: 0.1071\n",
            "Epoch 85/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.0963 - val_loss: 0.1077\n",
            "Epoch 86/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0961 - val_loss: 0.1075\n",
            "Epoch 87/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0960 - val_loss: 0.1074\n",
            "Epoch 88/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.0959 - val_loss: 0.1066\n",
            "Epoch 89/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0958 - val_loss: 0.1075\n",
            "Epoch 90/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0956 - val_loss: 0.1065\n",
            "Epoch 91/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0956 - val_loss: 0.1078\n",
            "Epoch 92/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0955 - val_loss: 0.1070\n",
            "Epoch 93/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0952 - val_loss: 0.1084\n",
            "Epoch 94/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0951 - val_loss: 0.1076\n",
            "Epoch 95/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0950 - val_loss: 0.1085\n",
            "Epoch 96/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0949 - val_loss: 0.1067\n",
            "Epoch 97/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0948 - val_loss: 0.1071\n",
            "Epoch 98/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0947 - val_loss: 0.1070\n",
            "Epoch 99/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0946 - val_loss: 0.1068\n",
            "Epoch 100/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0945 - val_loss: 0.1072\n",
            "Epoch 101/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0944 - val_loss: 0.1067\n",
            "Epoch 102/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0942 - val_loss: 0.1077\n",
            "Epoch 103/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.0943 - val_loss: 0.1067\n",
            "Epoch 104/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0941 - val_loss: 0.1068\n",
            "Epoch 105/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0941 - val_loss: 0.1073\n",
            "Epoch 106/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0939 - val_loss: 0.1068\n",
            "Epoch 107/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0938 - val_loss: 0.1061\n",
            "Epoch 108/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.0937 - val_loss: 0.1074\n",
            "Epoch 109/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0937 - val_loss: 0.1064\n",
            "Epoch 110/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0935 - val_loss: 0.1064\n",
            "Epoch 111/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0934 - val_loss: 0.1072\n",
            "Epoch 112/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0933 - val_loss: 0.1064\n",
            "Epoch 113/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0932 - val_loss: 0.1067\n",
            "Epoch 114/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0931 - val_loss: 0.1068\n",
            "Epoch 115/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0930 - val_loss: 0.1064\n",
            "Epoch 116/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0928 - val_loss: 0.1068\n",
            "Epoch 117/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0929 - val_loss: 0.1076\n",
            "Epoch 118/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0929 - val_loss: 0.1062\n",
            "Epoch 119/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0927 - val_loss: 0.1061\n",
            "Epoch 120/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0926 - val_loss: 0.1070\n",
            "Epoch 121/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.0925 - val_loss: 0.1074\n",
            "Epoch 122/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0925 - val_loss: 0.1059\n",
            "Epoch 123/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0924 - val_loss: 0.1080\n",
            "Epoch 124/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0923 - val_loss: 0.1062\n",
            "Epoch 125/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0922 - val_loss: 0.1067\n",
            "Epoch 126/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0921 - val_loss: 0.1058\n",
            "Epoch 127/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.0921 - val_loss: 0.1066\n",
            "Epoch 128/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0919 - val_loss: 0.1063\n",
            "Epoch 129/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0918 - val_loss: 0.1064\n",
            "Epoch 130/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0918 - val_loss: 0.1072\n",
            "Epoch 131/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0917 - val_loss: 0.1065\n",
            "Epoch 132/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0916 - val_loss: 0.1066\n",
            "Epoch 133/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0915 - val_loss: 0.1059\n",
            "Epoch 134/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0914 - val_loss: 0.1067\n",
            "Epoch 135/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0914 - val_loss: 0.1067\n",
            "Epoch 136/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0913 - val_loss: 0.1069\n",
            "Epoch 137/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0913 - val_loss: 0.1059\n",
            "Epoch 138/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0912 - val_loss: 0.1068\n",
            "Epoch 139/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0910 - val_loss: 0.1071\n",
            "Epoch 140/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0910 - val_loss: 0.1065\n",
            "Epoch 141/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0909 - val_loss: 0.1063\n",
            "Epoch 142/250\n",
            "2750/2750 [==============================] - 29s 11ms/step - loss: 0.0908 - val_loss: 0.1064\n",
            "Epoch 143/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0908 - val_loss: 0.1063\n",
            "Epoch 144/250\n",
            "2750/2750 [==============================] - 28s 10ms/step - loss: 0.0907 - val_loss: 0.1059\n",
            "Epoch 145/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0906 - val_loss: 0.1062\n",
            "Epoch 146/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0907 - val_loss: 0.1073\n",
            "Epoch 147/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0905 - val_loss: 0.1071\n",
            "Epoch 148/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0905 - val_loss: 0.1062\n",
            "Epoch 149/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0904 - val_loss: 0.1060\n",
            "Epoch 150/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0904 - val_loss: 0.1062\n",
            "Epoch 151/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0903 - val_loss: 0.1063\n",
            "Epoch 152/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0902 - val_loss: 0.1062\n",
            "Epoch 153/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0902 - val_loss: 0.1074\n",
            "Epoch 154/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0900 - val_loss: 0.1063\n",
            "Epoch 155/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.0900 - val_loss: 0.1067\n",
            "Epoch 156/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0899 - val_loss: 0.1072\n",
            "Epoch 157/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0900 - val_loss: 0.1059\n",
            "Epoch 158/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0898 - val_loss: 0.1063\n",
            "Epoch 159/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0899 - val_loss: 0.1062\n",
            "Epoch 160/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0897 - val_loss: 0.1060\n",
            "Epoch 161/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0895 - val_loss: 0.1069\n",
            "Epoch 162/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0895 - val_loss: 0.1073\n",
            "Epoch 163/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0895 - val_loss: 0.1063\n",
            "Epoch 164/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0895 - val_loss: 0.1063\n",
            "Epoch 165/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0894 - val_loss: 0.1062\n",
            "Epoch 166/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0893 - val_loss: 0.1066\n",
            "Epoch 167/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.0894 - val_loss: 0.1066\n",
            "Epoch 168/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0892 - val_loss: 0.1066\n",
            "Epoch 169/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0891 - val_loss: 0.1079\n",
            "Epoch 170/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0891 - val_loss: 0.1059\n",
            "Epoch 171/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0891 - val_loss: 0.1069\n",
            "Epoch 172/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0890 - val_loss: 0.1071\n",
            "Epoch 173/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0890 - val_loss: 0.1062\n",
            "Epoch 174/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0889 - val_loss: 0.1069\n",
            "Epoch 175/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0889 - val_loss: 0.1058\n",
            "Epoch 176/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0888 - val_loss: 0.1054\n",
            "Epoch 177/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0888 - val_loss: 0.1063\n",
            "Epoch 178/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0887 - val_loss: 0.1064\n",
            "Epoch 179/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0886 - val_loss: 0.1065\n",
            "Epoch 180/250\n",
            "2750/2750 [==============================] - 29s 11ms/step - loss: 0.0885 - val_loss: 0.1068\n",
            "Epoch 181/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0885 - val_loss: 0.1061\n",
            "Epoch 182/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0884 - val_loss: 0.1067\n",
            "Epoch 183/250\n",
            "2750/2750 [==============================] - 28s 10ms/step - loss: 0.0884 - val_loss: 0.1066\n",
            "Epoch 184/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0883 - val_loss: 0.1067\n",
            "Epoch 185/250\n",
            "2750/2750 [==============================] - 23s 8ms/step - loss: 0.0883 - val_loss: 0.1064\n",
            "Epoch 186/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0881 - val_loss: 0.1059\n",
            "Epoch 187/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0882 - val_loss: 0.1076\n",
            "Epoch 188/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0882 - val_loss: 0.1067\n",
            "Epoch 189/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0880 - val_loss: 0.1070\n",
            "Epoch 190/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0880 - val_loss: 0.1071\n",
            "Epoch 191/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0880 - val_loss: 0.1067\n",
            "Epoch 192/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0880 - val_loss: 0.1069\n",
            "Epoch 193/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0879 - val_loss: 0.1071\n",
            "Epoch 194/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0879 - val_loss: 0.1062\n",
            "Epoch 195/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0878 - val_loss: 0.1064\n",
            "Epoch 196/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0877 - val_loss: 0.1060\n",
            "Epoch 197/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0877 - val_loss: 0.1066\n",
            "Epoch 198/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0876 - val_loss: 0.1067\n",
            "Epoch 199/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0875 - val_loss: 0.1063\n",
            "Epoch 200/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0876 - val_loss: 0.1069\n",
            "Epoch 201/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0875 - val_loss: 0.1059\n",
            "Epoch 202/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0874 - val_loss: 0.1066\n",
            "Epoch 203/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0875 - val_loss: 0.1062\n",
            "Epoch 204/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0873 - val_loss: 0.1063\n",
            "Epoch 205/250\n",
            "2750/2750 [==============================] - 28s 10ms/step - loss: 0.0873 - val_loss: 0.1068\n",
            "Epoch 206/250\n",
            "2750/2750 [==============================] - 31s 11ms/step - loss: 0.0873 - val_loss: 0.1063\n",
            "Epoch 207/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0871 - val_loss: 0.1069\n",
            "Epoch 208/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0871 - val_loss: 0.1065\n",
            "Epoch 209/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0871 - val_loss: 0.1065\n",
            "Epoch 210/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0871 - val_loss: 0.1066\n",
            "Epoch 211/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0871 - val_loss: 0.1063\n",
            "Epoch 212/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.0870 - val_loss: 0.1073\n",
            "Epoch 213/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0869 - val_loss: 0.1065\n",
            "Epoch 214/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0869 - val_loss: 0.1063\n",
            "Epoch 215/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0869 - val_loss: 0.1071\n",
            "Epoch 216/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.0868 - val_loss: 0.1065\n",
            "Epoch 217/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0868 - val_loss: 0.1070\n",
            "Epoch 218/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0868 - val_loss: 0.1068\n",
            "Epoch 219/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0867 - val_loss: 0.1060\n",
            "Epoch 220/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0867 - val_loss: 0.1062\n",
            "Epoch 221/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0866 - val_loss: 0.1066\n",
            "Epoch 222/250\n",
            "2750/2750 [==============================] - 28s 10ms/step - loss: 0.0866 - val_loss: 0.1071\n",
            "Epoch 223/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0865 - val_loss: 0.1068\n",
            "Epoch 224/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0864 - val_loss: 0.1061\n",
            "Epoch 225/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0864 - val_loss: 0.1071\n",
            "Epoch 226/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0864 - val_loss: 0.1062\n",
            "Epoch 227/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.0864 - val_loss: 0.1070\n",
            "Epoch 228/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0862 - val_loss: 0.1074\n",
            "Epoch 229/250\n",
            "2750/2750 [==============================] - 36s 13ms/step - loss: 0.0862 - val_loss: 0.1066\n",
            "Epoch 230/250\n",
            "2750/2750 [==============================] - 35s 13ms/step - loss: 0.0863 - val_loss: 0.1068\n",
            "Epoch 231/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0862 - val_loss: 0.1063\n",
            "Epoch 232/250\n",
            "2750/2750 [==============================] - 27s 10ms/step - loss: 0.0862 - val_loss: 0.1065\n",
            "Epoch 233/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.0861 - val_loss: 0.1064\n",
            "Epoch 234/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.0860 - val_loss: 0.1064\n",
            "Epoch 235/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0859 - val_loss: 0.1068\n",
            "Epoch 236/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0861 - val_loss: 0.1074\n",
            "Epoch 237/250\n",
            "2750/2750 [==============================] - 29s 11ms/step - loss: 0.0859 - val_loss: 0.1069\n",
            "Epoch 238/250\n",
            "2750/2750 [==============================] - 24s 9ms/step - loss: 0.0859 - val_loss: 0.1064\n",
            "Epoch 239/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0858 - val_loss: 0.1066\n",
            "Epoch 240/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0859 - val_loss: 0.1068\n",
            "Epoch 241/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0858 - val_loss: 0.1059\n",
            "Epoch 242/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0857 - val_loss: 0.1071\n",
            "Epoch 243/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.0857 - val_loss: 0.1076\n",
            "Epoch 244/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0858 - val_loss: 0.1069\n",
            "Epoch 245/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.0856 - val_loss: 0.1068\n",
            "Epoch 246/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0856 - val_loss: 0.1074\n",
            "Epoch 247/250\n",
            "2750/2750 [==============================] - 26s 9ms/step - loss: 0.0856 - val_loss: 0.1074\n",
            "Epoch 248/250\n",
            "2750/2750 [==============================] - 29s 10ms/step - loss: 0.0855 - val_loss: 0.1065\n",
            "Epoch 249/250\n",
            "2750/2750 [==============================] - 25s 9ms/step - loss: 0.0855 - val_loss: 0.1066\n",
            "Epoch 250/250\n",
            "2750/2750 [==============================] - 26s 10ms/step - loss: 0.0854 - val_loss: 0.1068\n",
            "The time of execution of above program is : 6298.719681501389 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model"
      ],
      "metadata": {
        "id": "3lnB29-_cVp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_size = 16\n",
        "size = 16\n",
        "\n",
        "def rgb_L_ab(rgb_image, test_L):\n",
        "    lab_array = color.rgb2lab(np.array(rgb_image))# Convert RGB to LAB colorspace\n",
        "    lab_array[..., 0] = test_L #Change L channel to input\n",
        "    rgb_array = color.lab2rgb(lab_array)# Convert back LAB to RGB colorspace\n",
        "    rgb_array = (rgb_array * 255).astype(np.uint8)# Scale RGB values back to the range [0, 255]\n",
        "    rgb_array = Image.fromarray(rgb_array, mode='RGB')# Convert RGB array back to image\n",
        "    return rgb_array\n",
        "\n",
        "def crop_(im):\n",
        "    width, height = im.size\n",
        "    data = []\n",
        "    for j in range(0,int(height/n_size)):\n",
        "        for i in range(0,int(width/n_size)):\n",
        "            im1 = im.crop((0 + (n_size*i), 0 + (n_size*j), n_size + (n_size*i), n_size + (n_size*j)))\n",
        "            im1 = np.array(im1)\n",
        "            im1 = im1.astype(np.float32)\n",
        "            im1 = im1/255\n",
        "            data.append(im1)\n",
        "    return data\n",
        "\n",
        "#----------------READING THE TEST IMAGE--------------------#\n",
        "filename = str(\"[15]Come Hither-Before.png\")\n",
        "test = contrast_stretch(Image.open(file_path+ \"Testing/\" + filename).convert('RGB'))\n",
        "test_L = color.rgb2lab(np.array(test))\n",
        "test_L = test_L[..., 0]\n",
        "w_dirty, h_dirty = test.size\n",
        "#----------------------------------------------------------#\n",
        "\n",
        "autoencoder = keras.models.load_model(file_path + 'Model/unet_'+str(n_size)+'_rgb', custom_objects={'ssim_loss': ssim_loss})\n",
        "\n",
        "xx = int(w_dirty/n_size)\n",
        "final=[]\n",
        "\n",
        "for portion in range(0,xx):\n",
        "    im1 = test.crop((n_size*portion, 0, (n_size*portion) + n_size, h_dirty))\n",
        "    w1, h1 = im1.size\n",
        "    w = int(w1/n_size)\n",
        "    h = int(h1/n_size)\n",
        "    neverbeforeseen = np.array(crop_(im1))\n",
        "    decoded_imgs = autoencoder.predict(neverbeforeseen)\n",
        "    col = np.vstack((decoded_imgs[0],decoded_imgs[1]))\n",
        "    for i in range(2,h):\n",
        "        col = np.vstack((col,decoded_imgs[i]))\n",
        "    y = col\n",
        "    y = (y * 255).astype('uint8')\n",
        "    if portion == 0:\n",
        "        final = y\n",
        "    if portion > 0:\n",
        "        final = np.hstack((final,y))\n",
        "\n",
        "#----------------TEST ON FULL RGB--------------------#\n",
        "final_rgb = final\n",
        "final_rgb = np.squeeze(final_rgb)\n",
        "reconstructed_rgb = Image.fromarray(final_rgb)\n",
        "reconstructed_rgb.save(file_path + \"Testing/\"+\"Model_1_\"+filename)\n",
        "plt.figure(), plt.imshow(reconstructed_rgb)\n",
        "\n",
        "#----------------TEST ON L from input and AB from convert RGB2LAB of model--------------------#\n",
        "final_lab = rgb_L_ab(final, test_L)\n",
        "final_lab = np.squeeze(final_lab)\n",
        "reconstructed_lab = Image.fromarray(final_lab)\n",
        "reconstructed_lab.save(file_path + \"Testing/\"+\"Model_2_\"+filename)\n",
        "plt.figure(), plt.imshow(reconstructed_lab)"
      ],
      "metadata": {
        "id": "B9p3nBVSjRys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# Function to open image and convert to numpy array\n",
        "def load_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB mode\n",
        "    return np.array(image)\n",
        "\n",
        "image_path1 = file_path + 'Testing/[15]Come Hither-After.png'\n",
        "image_path2 = file_path + \"Testing/\"+\"Model_2_\"+filename\n",
        "\n",
        "# Load images\n",
        "image1 = load_image(image_path1)\n",
        "image2 = load_image(image_path2)\n",
        "\n",
        "# Ensure the images have the same dimensions\n",
        "if image1.shape != image2.shape:\n",
        "    raise ValueError(\"Input images must have the same dimensions.\")\n",
        "\n",
        "# Compute SSIM for each channel and average the results\n",
        "ssim_index_r = ssim(image1[:, :, 0], image2[:, :, 0], data_range=image1[:, :, 0].max() - image1[:, :, 0].min())\n",
        "ssim_index_g = ssim(image1[:, :, 1], image2[:, :, 1], data_range=image1[:, :, 1].max() - image1[:, :, 1].min())\n",
        "ssim_index_b = ssim(image1[:, :, 2], image2[:, :, 2], data_range=image1[:, :, 2].max() - image1[:, :, 2].min())\n",
        "\n",
        "ssim_index_rgb = (ssim_index_r + ssim_index_g + ssim_index_b) / 3\n",
        "\n",
        "print(f\"SSIM for R channel: {ssim_index_r}\")\n",
        "print(f\"SSIM for G channel: {ssim_index_g}\")\n",
        "print(f\"SSIM for B channel: {ssim_index_b}\")\n",
        "print(f\"Average SSIM for RGB image(Method4): {ssim_index_rgb}\")"
      ],
      "metadata": {
        "id": "hZXwetFIpIUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a3c6543-f17f-4642-dbd0-69931cfafdb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSIM for R channel: 0.7135495848132634\n",
            "SSIM for G channel: 0.7148738323294628\n",
            "SSIM for B channel: 0.6639641471293708\n",
            "Average SSIM for RGB image(Method4): 0.6974625214240323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNqhZzbSyA4S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}